---
title: "TMA4268 - Project 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries used throughout the exercise
```{r}
library(ggplot2)
library(palmerpenguins) # Contains the data set 'penguins'.
library(ggfortify)
library(tidyverse)
library(GGally)
library(MASS)
library(class)
library(base)

```



## Problem 2

### a)

One error made is excluding sex because of a low p-value, the p-value is the probability to observe a result equal or more extreme than the one we did, given that the null hypothesis $H_0: \beta = 0$, so a lower p-value means the result is more statistically significant.

Another error made is deciding whether to reject the null hypothesis that the species coefficient overall is actually zero based on the p-values of the coefficients. Since the null hypothesis in this case is $H_0: \beta_{Chinstrap} = \beta_{Gentoo} = 0$ at the same time. We need to do a F-test.

A third error made is claiming that chinstrap penguins are the largest since they have the largest coefficient. Having a large coefficient only means that for a given set of parameters a chinstrap penguin is going to be heavier than another type of penguin. And when we include interaction terms which changes the the slopes, a large coefficient only guarantees it is larger for a range of parameters near zero. And by looking at body mass of the penguins in the data set we find that gentoo penguins have the highest mean body mass.



### b)

```{r}

data(penguins)

Penguins <- subset(penguins, select = -c(island, year))
Penguins <- na.omit(Penguins)

ggplot(data = Penguins) +
  geom_histogram(binwidth = 40, mapping = aes(x = body_mass_g)) +
  facet_wrap(~ sex, nrow = 2)

ggplot(data = Penguins) +
  geom_histogram(binwidth = 40, mapping = aes(x = body_mass_g)) +
  facet_wrap(~ species, nrow = 3)

```

### c)

```{r}

Penguins <- subset(penguins, select = -c(island, year))

penguin.model <- lm(body_mass_g ~ flipper_length_mm + sex + bill_depth_mm * species,
                    data = Penguins)
summary(penguin.model)

anova(penguin.model)


autoplot(penguin.model, smooth.colour = NA)

```

## Problem 3. 

Now, the main idea is to classify the species of the penguins for a given body mass and flipper length. The dataset contains 3 type of penguin species, yet, classifying 3 penguin species is not straightforward. So, for simplicity, we define our goal to classify a penguin belonging to the species - Adelie, or not Adelie. This will give us two-class classification problem instead of three. 

Since, we will use only three parameters from the dataset, it is wise to create a small dataset, and work with it. First, the variables should be converted to numeric (because knn function cannot handle the int class, and generates errors) and removes any missing observsations. 

```{r}
penguins$adelie <- ifelse(penguins$species == "Adelie", 1, 0)

# Select only relevant variables and remove all rows with missing values in body
# mass, flipper length, sex or species.
Penguins_reduced <- penguins %>% dplyr::select(body_mass_g, flipper_length_mm,
                    adelie) %>%  mutate(body_mass_g = as.numeric(body_mass_g),
                    flipper_length_mm = as.numeric(flipper_length_mm)) %>% drop_na()

```
Now, we need two datasets: the one to train the model, then to test the model. It is common to divide 70% of dataset as a training dataset, and the rest will be a test dataset. 

```{r}
set.seed(4268)

# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

### a)

#### i) Logistic regression

```{r}
log_reg = glm(adelie ~ body_mass_g + flipper_length_mm, data = train, family = 
                "binomial")

summary(log_reg)$coef
```

Here, we train the model using train dataset, and the result is available above. 

```{r}
prob_test_lr <- predict(log_reg, newdata = test, type = "response")

pred_test_lr <- ifelse(prob_test_lr > 0.5, 1, 0)

```

predict() gives the probabilities of the species = Adelie (class = 1). Then, we apply a cutoff value of 0.5 to those probabilities, to get predicted classes.


```{r}

adelie_lr_df = bind_rows(mutate(test, pred_test_lr))
adelie_lr_df$pred_test_lr = as.factor(adelie_lr_df$pred_test_lr)

gg_p = ggplot(test, aes(x=flipper_length_mm, y=body_mass_g, color=adelie)) + 
      geom_point(aes(x = body_mass_g, y=flipper_length_mm, colour=pred_test_lr), 
      data=adelie_lr_df, size=2) + xlab("Flipper Length of a penguin (mm)") + 
      ylab("Body Mass of a penguin (g)") + 
      ggtitle("The classification of species using logistic regression") + theme_bw()       + theme(plot.title = element_text(hjust = 0.5)) 
gg_p
```


Then, we use test dataset for testing the model. The plot illustrates the testing result.

#### ii) Quadratic Discriminant Analysis

We do the same for classification using Quadratic Discriminant Analysis

```{r}
qda_reg = qda(adelie ~ body_mass_g + flipper_length_mm, data = train)

prob_test_qda <- predict(qda_reg, newdata = test, type = "response")$posterior
pred_test_qda <- predict(qda_reg, newdata = test, type = "response")$class

```

Because QDA identifies the probabilites of having species = Adelie (class = 1), and having species = not Adelie (class = 0), predict() function will give both probabilites and already cut-off value (0.5) applied predictions. So, $\$$posteriors will give the probabilities, $\$$class will give 0.5 cut-off value applied corresponding classes. 

```{r}
adelie_qda_df = bind_rows(mutate(test, pred_test_qda))
adelie_qda_df$pred_test_qda = as.factor(adelie_qda_df$pred_test_qda)

qda_plot = ggplot(test, aes(x=body_mass_g, y=flipper_length_mm, color=adelie)) +
          geom_point(aes(x = body_mass_g, y=flipper_length_mm, colour=pred_test_qda), 
          data=adelie_qda_df, size=2) + xlab("Flipper Length of a penguin (mm)") + 
          ylab("Body Mass of a penguin (g)") +
          ggtitle("The classification of species using Quadratic Discriminat Analysis")         + theme_bw() + theme(plot.title = element_text(hjust = 0.5))

qda_plot
```

Again, this plot illustrates the testing result.

#### iii) K-nearest Neighbors
```{r}
knnMod = knn(train = train, test = test, cl = train$adelie, k = 25, prob = T)

knn_r <- data.frame(data.matrix(knnMod), attributes(knnMod)$prob)
```

After having the model, we put the result in matrix form for better usage.

```{r}
probKNN = ifelse(knnMod == 0, 1 - attributes(knnMod)$prob, attributes(knnMod)$prob)
predKNN = ifelse(probKNN > 0.5, 1, 0)
```

This is the way to get the probabilities of species = Adelie (class = 1). Then, again we apply 0.5 cut-off value to get the predictions. 

```{r}
adelie_knn_df = bind_rows(mutate(test, probKNN, predKNN))
adelie_knn_df$predKNN = as.factor(adelie_knn_df$predKNN)

gg = ggplot(test, aes(x=body_mass_g, y=flipper_length_mm,
      colour=adelie))+geom_point(aes(x=body_mass_g, y=flipper_length_mm,
      colour=predKNN), data=adelie_knn_df, size=2) + 
      xlab("Flipper Length of a penguin (mm)") + ylab("Body Mass of a penguin (g)") +
      ggtitle("The classification of species using \n K-nearest Neighbors") +
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

gg
```

#### iv) Sensitivity $\&$ specificity

For Logistic regression

```{r}

predictions_log_reg = data.frame(prob_test_lr, pred_test_lr, test[, "adelie"])
colnames(predictions_log_reg) = c("Estim. prob. of Y=1", "Predicted class",
                             "True class")
head(predictions_log_reg)

conf_mat_log_r = table(predicted = predictions_log_reg["Predicted class"][,1], true = 
        predictions_log_reg["True class"][,1] )

print("The confusion matrix is:") 
conf_mat_log_r

test_error_rate_lr = (conf_mat_log_r[1, 2] + conf_mat_log_r[2, 1])/sum(conf_mat_log_r)
print(paste( "The test error for Logistic regression -", signif(test_error_rate_lr, digits = 4))) 

sensitivity_lr = conf_mat_log_r[2, 2]/(conf_mat_log_r[1, 2]+conf_mat_log_r[2, 2])
specificity_lr = conf_mat_log_r[1, 1]/(conf_mat_log_r[2, 1]+conf_mat_log_r[1, 1])

print(paste( "The sensitivity - ", signif(sensitivity_lr, digits=4))) 
print(paste( "The specificity - ", signif(specificity_lr, digits=4))) 

```

For Quadratic Discriminant Analysis

```{r}
predictions_qda = data.frame(pred_test_qda, test[, "adelie"])
colnames(predictions_qda) = c("Predicted class","True class")
head(predictions_qda)

conf_mat_qda = table(predicted = predictions_qda["Predicted class"][,1], true = 
        predictions_qda["True class"][,1] )
print("The confusion matrix is:") 
conf_mat_qda

test_error_rate_qda = (conf_mat_qda[1, 2] + conf_mat_qda[2, 1])/sum(conf_mat_qda)
print(paste( "The test error for Quadratic Discriminant Analysis -", signif(test_error_rate_qda, digits = 4))) 

sensitivity_qda = conf_mat_qda[2, 2]/(conf_mat_qda[1, 2]+conf_mat_qda[2, 2])
specificity_qda = conf_mat_qda[1, 1]/(conf_mat_qda[2, 1]+conf_mat_qda[1, 1])

print(paste( "The sensitivity - ", signif(sensitivity_qda, digits=4))) 
print(paste( "The specificity - ", signif(specificity_qda, digits=4))) 

```

For K-nearest Neighbors

```{r}
predictions_knn = data.frame(knn_r[, 2], knn_r[, 1], test[, "adelie"])
colnames(predictions_knn) = c("Estim. prob. of Y=1","Predicted class","True class")
head(predictions_knn)

conf_mat_knn = table(predicted = predictions_knn["Predicted class"][,1], true = 
        predictions_knn["True class"][,1] )
print("The confusion matrix is:") 
conf_mat_knn

test_error_rate_knn = (conf_mat_knn[1, 2] + conf_mat_knn[2, 1])/sum(conf_mat_knn)
print(paste( "The test error for K-nearest Neighbors -", signif(test_error_rate_knn, digits = 4))) 

sensitivity_knn = conf_mat_knn[2, 2]/(conf_mat_knn[1, 2]+conf_mat_knn[2, 2])
specificity_knn = conf_mat_knn[1, 1]/(conf_mat_knn[2, 1]+conf_mat_knn[1, 1])

print(paste( "The sensitivity - ", signif(sensitivity_knn, digits=4))) 
print(paste( "The specificity - ", signif(specificity_knn, digits=4)))
```
### B. ROC $\&$ AUC

#### i)
```{r}
lr_roc = roc.curve(test$adelie, prob_test_lr, col = "blue", lwd=3, 
                   xlab = "1 - specificity", ylab = "sensitivity")

qda_roc = roc.curve(test$adelie, prob_test_qda[,2], col = "red", lwd=3, add = T)

probKNN = ifelse(knnMod == 0, 1 - attributes(knnMod)$prob, attributes(knnMod)$prob)

knn_roc = roc.curve(test$adelie, probKNN, col = "green", lwd=3, add = T)

legend("bottomright", legend = c("Logistic Regression", "Quadradic Discriminant \nAnalysis", "K-nearest Neighbors"), cex=0.8, lwd = 3, 
       col = c("blue", "red", "green"))

```
Areas Under Curves: 

```{r}

print(paste( "AUC of Logistic Regression ", signif(lr_roc$auc, digits = 4)))
print(paste( "AUC of Quadradic Discriminant Analysis ", signif(qda_roc$auc, digits = 4)))
print(paste( "AUC of K-nearest Neighbors ", signif(knn_roc$auc, digits = 4)))
```

#### ii) 
 
ROC curve - Receiver operating characteristics curve displays the relation between False Positive rate and True Positive rate for all possible threshold values (0 to 1). Since True Positive rate is $\frac{True Positives}{All Positives}$, it is expressed as sensitivity, and False Positive rate as 1 - Specificity. If the model is ideal model, the sensitivity = 1, and specificity = 1, so, the curve will be on the top left. While a straight line will represent a model with random guesses on the outcome. 

In order to check the overall performance of the model, AUC (Area Under Curve) values are compared, meaning that if AUC value is high, that means the curve is closer to the ideal situation. 

<h1><center> The comparison of the models we have: Logistic regression, Quadratic Discriminant Analysis, K-nearest Neighbors </center></h1>

When we look at the ROC Curves, we clearly see that the model that performs worse than others is K-nearest Neighbors. This is proved by AUC area. (AUC area of KNN = 0.8417). 

The performance of Logistic regression and Quadratic Discriminant Analysis is almost the same. Yet, there is a slight difference in AUC areas. Based on AUC areas, Logistic regression performs a bit better than Quadratic Discriminant Analysis (AUC area of LogReg = 0.9391, AUC area of QDA = 0.938)


### C. 

$odds = \frac{p_i}{1-p_i} = \frac{\text{P}(Y_i=1 \mid X=x)}{\text{P}(Y_i=0 \mid X = x)} = exp(\beta_0) \cdot exp(\beta_1x_1) \cdot exp(\beta_2x_2)$

$odds\ ratio = \frac{odds[Y_i=1 \mid X=x_1+1000]}{odds[Y_i=1 \mid X=x_1]} = \frac{exp(\beta_0) \cdot exp(\beta_1(x_1+1000)) \cdot exp(\beta_2x_2)}{exp(\beta_0) \cdot exp(\beta_1x_1) \cdot exp(\beta_2x_2)} = exp(\beta_1*1000)$

We find $\beta_1$ from coefficient table of Logistic regression

$odds\ ratio = $
```{r}
summary(log_reg)$coef

```

So, $\beta_1 = 0.000712$

```{r}

print(paste( "The odds will be multiplied by ", signif(exp(coef(log_reg)[2]*1000), digits = 4)))

```
The answer is III. 

### D.


